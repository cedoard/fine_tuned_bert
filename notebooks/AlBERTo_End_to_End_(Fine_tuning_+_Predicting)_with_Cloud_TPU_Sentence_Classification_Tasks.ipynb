{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlBERTo_End_to_End_(Fine_tuning_+_Predicting)_with_Cloud_TPU_Sentence_Classification_Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedoard/fine_tuned_bert/blob/master/notebooks/AlBERTo_End_to_End_(Fine_tuning_%2B_Predicting)_with_Cloud_TPU_Sentence_Classification_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHQH4OCHZ9bq"
      },
      "source": [
        "# Original code licensed by:\n",
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkTLZ3I4_7c_"
      },
      "source": [
        "# AlBERTo End to End (Fine-tuning + Predicting) with Cloud TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtjs1QDb3DX"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**BERT**, or **B**idirectional **E**mbedding **R**epresentations from **T**ransformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. The academic paper can be found here: https://arxiv.org/abs/1810.04805.\n",
        "\n",
        "In particular we use this Notebook for fine-tuning **AlBERTo**, the first italian undertanding language model for Twitter Language.\n",
        "\n",
        "**Note:**  You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud \n",
        "Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) for how to create GCP account and GCS bucket. \n",
        "\n",
        "You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-JXlueIuPH"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkof5uHaQ_c"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n",
        "\n",
        "   1. Create a Cloud Storage bucket for your TensorBoard logs at http://console.cloud.google.com/storage and fill in the BUCKET parameter in the \"Parameters\" section below.\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr29zwkRTvZg"
      },
      "source": [
        "### Install/Import required modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uktgjI7JT7vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b53cfcc-2224-46f9-dc27-bf16b3539ad6"
      },
      "source": [
        "!pip install tensorflow==1.14.0\r\n",
        "\r\n",
        "!pip install gcsfs \r\n",
        "!pip install fsspec \r\n",
        "\r\n",
        "!pip install ekphrasis\r\n",
        "#!pip install pandas\r\n",
        "#!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 86kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.19.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/85/75/3d669945d41e5aedd5c4333b9dc6192b7839d2bafd04b75b8222d4e92ae0/gcsfs-0.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.17.2)\n",
            "Collecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.2)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (50.3.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (20.3.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Building wheels for collected packages: idna-ssl\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=9c165afd308b16ecbffad6972d64be2b91ddd8cd6bd78cacd819236a78f91212\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built idna-ssl\n",
            "Installing collected packages: fsspec, multidict, yarl, async-timeout, idna-ssl, aiohttp, gcsfs\n",
            "Successfully installed aiohttp-3.7.3 async-timeout-3.0.1 fsspec-0.8.5 gcsfs-0.7.1 idna-ssl-1.1.0 multidict-5.1.0 yarl-1.6.3\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.6/dist-packages (0.8.5)\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/84/e039c6ffc6603f2dfe966972d345d4f650a4ffd74b18c852ece645de12ac/ujson-4.0.1-cp36-cp36m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.19.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82844 sha256=707c5905bc7442a717f4cb9bdd2824af784359f23c52bba382fa69d9fd74e493\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=c13ba26c30c1411bfeaca7566d6f3de02c1eeb514f89afb5f11d1a7ae6a8cee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.8 ujson-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXWV6SARX2zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714589f3-57c4-4000-e236-3ac689e07614"
      },
      "source": [
        "import datetime\r\n",
        "import json\r\n",
        "import os\r\n",
        "import pprint\r\n",
        "import random\r\n",
        "import string\r\n",
        "import sys\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "#PREPARE TRAINING SENTENCES\r\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\r\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\r\n",
        "from ekphrasis.dicts.emoticons import emoticons\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdMmwCJFaT8F"
      },
      "source": [
        "\r\n",
        "### Set up your TPU environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPoDdlBRTGR"
      },
      "source": [
        "\r\n",
        "Google Cloud Shell commands (see: [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart))\r\n",
        "\r\n",
        "`export PROJECT_ID=reberting`\r\n",
        "\r\n",
        "`gcloud config set project $PROJECT_ID`\r\n",
        "\r\n",
        "`gsutil mb -p ${PROJECT_ID} -c standard -l us-central1 -b on gs://bucket-rebert`\r\n",
        "\r\n",
        "```\r\n",
        "ctpu up --project=${PROJECT_ID} \\\r\n",
        " --zone=us-central1-b \\\r\n",
        " --tf-version=1.14 \\\r\n",
        " --name=tpu-alberto\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk6xSjIhTdDN"
      },
      "source": [
        "\r\n",
        "In this section, you perform the following tasks:\r\n",
        "\r\n",
        "*   Set up a Colab TPU running environment\r\n",
        "*   Verify that you are connected to a TPU device\r\n",
        "*   Upload your credentials to TPU to access your GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb771c6-6a6a-4cbb-b374-1a0f02f297f5"
      },
      "source": [
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.124.11.2:8470\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 11138662374282293171),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7478276998692968534),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17842271090012816987),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10636817016454091163),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14874749825002370954),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12262230757101204018),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17530365606553092247),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4325664238015414157),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 506650877169678595),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8811552168619247794),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16704767274748533099)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBP35oCDmbF"
      },
      "source": [
        "### Prepare and import BERT modules\n",
        "​\n",
        "With your environment configured, you can now prepare and import the BERT modules. The following step clones the source code from GitHub and import the modules from the source. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a66d7b-f306-4c93-aa77-1db13bfc8f0b"
      },
      "source": [
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "# import python modules defined by BERT\n",
        "from run_classifier import *\n",
        "import modeling\n",
        "import optimization\n",
        "import tokenization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert_repo'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 315.49 KiB | 4.15 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n",
            "WARNING:tensorflow:From bert_repo/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud4ZSFOT3zTQ"
      },
      "source": [
        "### Define Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7i9nNFq4gI4",
        "outputId": "a56492b3-ca85-439e-d74b-4e43eb40990e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/data/\"\r\n",
        "MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/model/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063a3a9e-c4a2-4c2f-8d2f-baa19df1c803"
      },
      "source": [
        "TASK = 'SENTIPOLC_TASK3' #@param {type:\"string\"}\n",
        "BUCKET = 'bucket-rebert' #@param {type:\"string\"}\n",
        "INIT_MODEL = 'alberto_model.ckpt'\n",
        "\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "BUCKET_DIR = 'gs://{}'.format(BUCKET)\n",
        "\n",
        "OUTPUT_DIR = 'gs://{}/{}/models/'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "#CONFIGURE AlBERTo MODEL\n",
        "BERT_CONFIG_FILE = 'gs://{}/config.json'.format(BUCKET) #@param {type:\"string\"}\n",
        "VOCAB_FILE = 'gs://{}/vocab.txt'.format(BUCKET) #@param {type:\"string\"}\n",
        "#VOCAB_FILE = os.path.join(DATA_PATH,\"vocab.txt\")\n",
        "\n",
        "tf.gfile.MakeDirs('gs://{}/alberto_model'.format(BUCKET))\n",
        "INIT_CHECKPOINT = 'gs://{}/alberto_model/{}'.format(BUCKET,INIT_MODEL) #@param {type:\"string\"}\n",
        "#INIT_CHECKPOINT = os.path.join(MODEL_PATH,'SENTIPOLC_TASK2_NEG_N','model.ckpt-44')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bucket-rebert/SENTIPOLC_TASK3/models/ *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ_LHyzcoYW"
      },
      "source": [
        "### Initialize BERT hyperparams and initialize TPU config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYVYULZiKvUi"
      },
      "source": [
        "#SET THE PARAMETERS\n",
        "TRAIN_BATCH_SIZE = 512\n",
        "PREDICT_BATCH_SIZE = 512\n",
        "EVAL_BATCH_SIZE = 512\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAadlolf59wM",
        "outputId": "3048d90b-703e-42bd-caa0-27826de33dc1"
      },
      "source": [
        "#Labels used for annotating sentences\r\n",
        "label_list = [0, 1]\r\n",
        "\r\n",
        "#Inizialize BERT tokenizer\r\n",
        "tokenizer = tokenization.FullTokenizer(VOCAB_FILE, do_lower_case=True)\r\n",
        "tokenizer.tokenize(\"dovevo arrivare in università luiss e si è spento perché è entrato nella zona in cui non può più circolare ma va va\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert_repo/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dovevo',\n",
              " 'arrivare',\n",
              " 'in',\n",
              " 'universita',\n",
              " 'luiss',\n",
              " 'e',\n",
              " 'si',\n",
              " 'e',\n",
              " 'spento',\n",
              " 'perche',\n",
              " 'e',\n",
              " 'entrato',\n",
              " 'nella',\n",
              " 'zona',\n",
              " 'in',\n",
              " 'cui',\n",
              " 'non',\n",
              " 'puo',\n",
              " 'piu',\n",
              " 'circolare',\n",
              " 'ma',\n",
              " 'va',\n",
              " 'va']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bNb-at24ONc"
      },
      "source": [
        "### Prepare the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzfl2-tcQUc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08b3658-1c4f-4fa7-89ad-ddf7d229e9d9"
      },
      "source": [
        "text_processor = TextPreProcessor (\n",
        "    # terms that will be normalized\n",
        "    normalize=[ 'url' , 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'] ,\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\"} ,\n",
        "    fix_html=True ,  # fix HTML tokens\n",
        "\n",
        "    unpack_hashtags=True ,  # perform word segmentation on hashtags\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts = [ emoticons ]\n",
        ")\n",
        "\n",
        "def ekphrasis_preprocess(text_processor, s):\n",
        "  s = s.lower()\n",
        "  s = str(\" \".join(text_processor.pre_process_doc(s)))\n",
        "  s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n",
        "  s = re.sub(r\"\\s+\", ' ', s)\n",
        "  s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n",
        "  s = re.sub ( r'^\\s' , '' , s )\n",
        "  s = re.sub ( r'\\s$' , '' , s )\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n",
            "Reading english - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_2grams.txt\n",
            "Reading english - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "m6oNo17K1KD3",
        "outputId": "4539d8de-e183-473f-ec87-fba424e2b678"
      },
      "source": [
        "#LOAD TRAINING AND TEST DATA\r\n",
        "training_data = pd.read_excel(os.path.join(BUCKET_DIR,'rev_df_final.xlsx'))\r\n",
        "training_data = training_data.loc[~training_data.sentiment.isin(['NEUTRAL'])]\r\n",
        "training_data = training_data.dropna().reset_index(drop=True)\r\n",
        "\r\n",
        "sentences = training_data.iloc[:,0]\r\n",
        "labels_str = training_data.iloc[:,-1]\r\n",
        "print(len(sentences))\r\n",
        "print(labels_str.nunique())\r\n",
        "\r\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7306\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>date</th>\n",
              "      <th>keywords</th>\n",
              "      <th>rating</th>\n",
              "      <th>username</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rasentiamo il ridicolo. Il servizio e l'idea s...</td>\n",
              "      <td>2020-11-05 15:34:53</td>\n",
              "      <td>helbiz</td>\n",
              "      <td>2</td>\n",
              "      <td>Roberto Spinelli</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trovo ottima questa iniziativa . Purtroppo le ...</td>\n",
              "      <td>2020-11-03 11:25:06</td>\n",
              "      <td>helbiz</td>\n",
              "      <td>4</td>\n",
              "      <td>graziano Qutro</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Monopattini regolarmente parcheggiati, ma che ...</td>\n",
              "      <td>2020-10-28 17:07:03</td>\n",
              "      <td>helbiz</td>\n",
              "      <td>1</td>\n",
              "      <td>Fabio C</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dopo aver messo due (letteralmente due) bici a...</td>\n",
              "      <td>2020-10-21 20:50:21</td>\n",
              "      <td>helbiz</td>\n",
              "      <td>1</td>\n",
              "      <td>Antonio Casto</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dopo i primi mesi in cui il servizio era relat...</td>\n",
              "      <td>2020-11-01 12:13:48</td>\n",
              "      <td>helbiz</td>\n",
              "      <td>1</td>\n",
              "      <td>Franco Papalia</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  ... sentiment\n",
              "0  Rasentiamo il ridicolo. Il servizio e l'idea s...  ...       NEG\n",
              "1  Trovo ottima questa iniziativa . Purtroppo le ...  ...       POS\n",
              "2  Monopattini regolarmente parcheggiati, ma che ...  ...       NEG\n",
              "3  Dopo aver messo due (letteralmente due) bici a...  ...       NEG\n",
              "4  Dopo i primi mesi in cui il servizio era relat...  ...       NEG\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBoMdhmX3NwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1473c7a3-a06c-4552-8030-f85a8baa5045"
      },
      "source": [
        "#PREPROCESS TRAINING AND TEST DATA\n",
        "def func(row):\n",
        "    if row == 'POS':\n",
        "        return 1\n",
        "    elif row =='NEG':\n",
        "        return 0\n",
        "\n",
        "labels = list(map(lambda x: func(x),labels_str))\n",
        "print(labels[:20])\n",
        "\n",
        "sentences_filtered = []\n",
        "i = 0\n",
        "for s in sentences:\n",
        "  sentences_filtered.append([labels[i],ekphrasis_preprocess(text_processor, s)])\n",
        "  i = i+1\n",
        "\n",
        "np.array(sentences_filtered)\n",
        "\n",
        "np.random.shuffle(sentences_filtered)\n",
        "split = int(len(sentences_filtered)*0.8)\n",
        "sentences_filtered_train, sentences_filtered_test = sentences_filtered[:split], sentences_filtered[split:]\n",
        "\n",
        "print(len(sentences_filtered),len(sentences_filtered_train),len(sentences_filtered_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n",
            "7306 5844 1462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYdyQA183LCq",
        "outputId": "ee644605-2e9f-4f4e-fd51-80c6913ba62f"
      },
      "source": [
        "'''\r\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExample's using the constructor provided in the BERT library.\r\n",
        "\r\n",
        "    text_a is the text we want to classify, which in this case, is the Request field in our Dataframe.\r\n",
        "    text_b is used if we're training a model to understand the relationship between sentences (i.e. is text_b a translation of text_a? Is text_b an answer to the question asked by text_a?). This doesn't apply to our task, so we can leave text_b blank.\r\n",
        "    label is the label for our example, i.e. True, False\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "f = lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\r\n",
        "                           text_a = x[1], \r\n",
        "                           text_b = None, \r\n",
        "                           label = int(x[0])\r\n",
        "                           )\r\n",
        "\r\n",
        "f2 = lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\r\n",
        "                            text_a = x[1], \r\n",
        "                            text_b = None, \r\n",
        "                            label = 0\r\n",
        "                            )\r\n",
        "\r\n",
        "train_examples = map(f,sentences_filtered_train)\r\n",
        "train_examples = list(train_examples)\r\n",
        "train_examples = np.array(train_examples)\r\n",
        "\r\n",
        "test_examples = map(f2,sentences_filtered_test)\r\n",
        "test_examples = list(test_examples)\r\n",
        "test_examples = np.array(test_examples)\r\n",
        "\r\n",
        "print(test_examples.shape)\r\n",
        "print(train_examples.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1462,)\n",
            "(5844,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8snKD00ZOVNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07322be3-04f9-4e1d-b235-15b4fa562bc4"
      },
      "source": [
        "#Test data just created\n",
        "for r in test_examples[:10]:\n",
        "  print(r.text_a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "totalmente sconsigliata ! ció che mi é successo stamattina ha del comico dovevo prendere il treno in centrale alle <number> e <number> noleggio il motorino alle <number> e <number> ma l app non funziona rimane bloccata alla schermata di noleggio dopo <number> minuti fermo la dove gia immaginavo di perdere il treno a causa di mimoto passa il tram lo vedo e corro a prenderlo dopo <number> minuti che sono sul tram nimoto da sola inizia il noleggio ed apre il bauletto del motorino ho dovuto pagare <number> euro più penali ridicoli\n",
            "ottimo servizio a milano e ottima la possibilità di usarlo in due con due caschi ! codice dxsjm per <money> gratis\n",
            "great ! the best would be if you integrate a navigator in the map\n",
            "bella l app bello il servizio che funziona ed è molto comodo a roma se volete <number> minuti gratis questo è il codice hcgsx\n",
            "un app fantastica e anche l idea lo è altrettanto ! grazie a lime ci si può muovere più velocemente tra le strade di torino e altre città senza troppo ingombro e senza problemi ! la consiglio a tutti il costo è di euro per sblocco e in seguito <number> eur al minuto qui a torino ma il prezzo può variare di città in città con il seguente codice invito potete avere uno sblocco gratuito r jim\n",
            "ti dice inizialmente che la tua città non offre il servizio quando poi dalla lista dice tu sei qui ti mostra quindi le stazioni presenti ma appena zoommi ti dice di zoommare di più ! ma come ? ma se con lo zoom più indietro me le facevi vedere ! ma soprattutto dice che ci sono le bici quando in realtà non ci sono mi è toccato correre all altra stazione ed ero di fretta un servizio sul quale non si può fare affidamento\n",
            "andavo al monopattino indicato ma puntualmente non c era penso che in molti lo lasciano nei cortili o parcheggi\n",
            "mi sono inscritta e registratta subito mi hanno tolto <number> euro non so perche ? ? e fino adesso non riesco a entrare nell app mandato copia documento patente e niente ce qualcosa che non va altre scooter noleggio non fanno tanto casino e problemi io le uso e volevo provare questo ma e non riesco voglio i <number> euro che mi avete tolto\n",
            "stupenda ! costa poco e il monopattino è una scheggia !\n",
            "ho preso oggi il monopattino più volte monza mi sono trovata abbastanza bene quando lo prendi ti dice costo di sblocco <money> per poi scalarti <money> quando lo lasci al parcheggio peccato per i pochi parcheggi comodo averne qualcuno anche fuori dal centro nel complesso <number> stelle le lascio volentieri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cSeUTV0CXB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45619695-512a-4346-8be0-dbcc80a2178d"
      },
      "source": [
        "'''\r\n",
        "Il metodo \"convert_examples_to_features\" crea le features da dare in input alla rete BERT:\r\n",
        "  - restituisce un array di oggetti \"InputFeatures\"\r\n",
        "  - \"InputFeatures\" ha i seguenti attributi:\r\n",
        "          - input_ids\r\n",
        "          - input_mask\r\n",
        "          - segment_ids\r\n",
        "          - label_id\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "train_features = convert_examples_to_features(\r\n",
        "      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert_repo/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Writing example 0 of 5844\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] rk ##ur a codice sconto molto comodo al centro di roma da evitare i s pietri ##ni privilegi ##are strade asfaltate [SEP]\n",
            "INFO:tensorflow:input_ids: 2 51760 7015 14 2242 2062 156 3727 55 631 12 65 45 2140 31 164 44508 909 7476 4300 2334 83887 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] comoda e di semplice utilizzo [SEP]\n",
            "INFO:tensorflow:input_ids: 2 10752 13 12 1038 6770 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] facilita di utilizzare non tanto facile rep ##eri ##rlo [SEP]\n",
            "INFO:tensorflow:input_ids: 2 30375 12 7522 19 136 944 8164 9290 2416 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] abbonamento mobi ##ke annuale perduto [SEP]\n",
            "INFO:tensorflow:input_ids: 2 5364 40546 8744 10457 5496 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] molto buona ma dovreste ampliare i metodi di pagamento [SEP]\n",
            "INFO:tensorflow:input_ids: 2 156 159 29 5214 37762 31 10787 12 4629 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjL1ltd0Cpza",
        "outputId": "52738dd7-27e2-4005-c658-daeeb39f3f55"
      },
      "source": [
        "print(train_examples[0].text_a)\r\n",
        "print(train_features[0].input_ids)\r\n",
        "print(train_features[0].input_mask)\r\n",
        "print(train_features[0].segment_ids)\r\n",
        "print(train_features[0].label_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rkur a codice sconto molto comodo al centro di roma da evitare i s pietrini privilegiare strade asfaltate\n",
            "[2, 51760, 7015, 14, 2242, 2062, 156, 3727, 55, 631, 12, 65, 45, 2140, 31, 164, 44508, 909, 7476, 4300, 2334, 83887, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3iFMeqLaSll"
      },
      "source": [
        "### Fine-tune pretrained BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXyJRc0OIHEU"
      },
      "source": [
        "This section demonstrates fine-tuning from a pre-trained BERT TF Hub module and running predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG9AEqHwtUZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619f6ce3-8ed4-4798-dd96-2140b853d7d1"
      },
      "source": [
        "BERT_CONFIG= modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\r\n",
        "\r\n",
        "#inizialize parameters\r\n",
        "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)+1\r\n",
        "num_warmup_steps = int(NUM_TRAIN_EPOCHS * WARMUP_PROPORTION)\r\n",
        "print(num_train_steps)\r\n",
        "print(num_warmup_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwcsdbLuIX2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bcaf4cd-a5bc-4a71-9f3c-20cafeb4350a"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  bert_config=BERT_CONFIG,\n",
        "  num_labels=len(label_list),\n",
        "  init_checkpoint=INIT_CHECKPOINT,\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  use_one_hot_embeddings=True\n",
        ")\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f478e1a1ea0>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bucket-rebert/SENTIPOLC_TASK3/models/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.96.217.26:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f478e0245f8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.96.217.26:8470', '_evaluation_master': 'grpc://10.96.217.26:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f47deccfb00>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaR8i758-Ijt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62cc946-a971-42e9-d0f2-e4e0ff63fbdd"
      },
      "source": [
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\", len(train_examples))\n",
        "print(\"  Num labels = %d\", len(label_list))\n",
        "print(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "print(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "train_input_fn = input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = %d 5844\n",
            "  Num labels = %d 2\n",
            "  Batch size = %d 512\n",
            "  Num steps = %d 115\n",
            "***** Started training at 2020-12-20 22:32:29.806059 *****\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "***** Finished training at 2020-12-20 22:32:30.550843 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRIqGLNI2ZWr"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edHti7HIMO5r"
      },
      "source": [
        "#SAVE MODEL TO PB FORMAT\r\n",
        "\r\n",
        "EXPORT_PATH_MODEL = os.path.join(BUCKET_DIR,'model_alberto_addestrato')\r\n",
        "#TO DO PROVA A TOGLIERE NONE\r\n",
        "def serving_input_fn():\r\n",
        "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\r\n",
        "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\r\n",
        "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\r\n",
        "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\r\n",
        "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\r\n",
        "        'label_ids': label_ids,\r\n",
        "        'input_ids': input_ids,\r\n",
        "        'input_mask': input_mask,\r\n",
        "        'segment_ids': segment_ids,\r\n",
        "    })()\r\n",
        "    return input_fn\r\n",
        "\r\n",
        "estimator._export_to_tpu = False\r\n",
        "estimator.export_saved_model(EXPORT_PATH_MODEL, serving_input_receiver_fn=serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fGBOIYwpNqi",
        "outputId": "6947a853-f278-4784-fd1f-ca6770796fd6"
      },
      "source": [
        "!gsutil cp -r \\\r\n",
        "  gs://bucket-rebert/model_alberto_addestrato/1608503656/ \\\r\n",
        "  /content/drive/My Drive/Colab Notebooks/model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: Destination URL must name a directory, bucket, or bucket\n",
            "subdirectory for the multiple source form of the cp command.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wuPqiizU25K",
        "outputId": "50567470-5080-40d3-abf9-b51aee5d7567"
      },
      "source": [
        "!saved_model_cli show --dir 'gs://bucket-rebert/model_alberto_addestrato/1608504002' --all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['input_ids'] tensor_info:\n",
            "        dtype: DT_INT32\n",
            "        shape: (-1, -1)\n",
            "        name: input_ids_1:0\n",
            "    inputs['input_mask'] tensor_info:\n",
            "        dtype: DT_INT32\n",
            "        shape: (-1, -1)\n",
            "        name: input_mask_1:0\n",
            "    inputs['label_ids'] tensor_info:\n",
            "        dtype: DT_INT32\n",
            "        shape: (-1)\n",
            "        name: label_ids_1:0\n",
            "    inputs['segment_ids'] tensor_info:\n",
            "        dtype: DT_INT32\n",
            "        shape: (-1, -1)\n",
            "        name: segment_ids_1:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['probabilities'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 2)\n",
            "        name: loss/Softmax:0\n",
            "  Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_04GUPx2knI"
      },
      "source": [
        "## Make predictions from loaded model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YjOFTm46rUY"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFLUDq_nllHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a00663-52e0-438b-a2b4-91623fce6d8b"
      },
      "source": [
        "#LOAD MODEL\r\n",
        "#LOAD_PATH = os.path.join(MODEL_PATH,'model_alberto_addestrato.h5','1608370941')\r\n",
        "#LOAD_PATH_PB = os.path.join(MODEL_PATH,'model_alberto_addestrato.h5','1608370941','saved_model.pb')\r\n",
        "LOAD_PATH_GCP = os.path.join(BUCKET_DIR,'model_alberto_addestrato','1608503656')\r\n",
        "LOAD_PATH_GCP_PB = os.path.join(BUCKET_DIR,'model_alberto_addestrato.h5','1608503656','saved_model.pb')\r\n",
        "print(LOAD_PATH_GCP)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://bucket-rebert/model_alberto_addestrato/1608503656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_8LJhN4bKyF",
        "outputId": "337b1ca8-d6f8-4efd-d9ad-201c3091dfbf"
      },
      "source": [
        "from tensorflow.contrib import predictor\r\n",
        "\r\n",
        "predict_fn = predictor.from_saved_model(LOAD_PATH_GCP)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from gs://bucket-rebert/model_alberto_addestrato/1608503656/variables/variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrGsH5Zu3JCt"
      },
      "source": [
        "### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg02dbr-Hnq_"
      },
      "source": [
        "def convert_single_string_to_input_dict(tokenizer,example_string_prep):\r\n",
        "\r\n",
        "  token_a = tokenizer.tokenize(example_string_prep)\r\n",
        "\r\n",
        "  tokens = []\r\n",
        "  segments_ids = []\r\n",
        "  segment_ids = []\r\n",
        "\r\n",
        "  tokens.append(\"[CLS]\")\r\n",
        "  segment_ids.append(0)\r\n",
        "  for token in token_a:\r\n",
        "    tokens.append(token)\r\n",
        "    segment_ids.append(0)\r\n",
        "\r\n",
        "  tokens.append('[SEP]')\r\n",
        "  segment_ids.append(0)\r\n",
        "    \r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "  input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "  while len(input_ids) < MAX_SEQ_LENGTH:\r\n",
        "    input_ids.append(0)\r\n",
        "    input_mask.append(0)\r\n",
        "    segment_ids.append(0)\r\n",
        "\r\n",
        "  label_id = [0]\r\n",
        "  padding = [0] * MAX_SEQ_LENGTH\r\n",
        "  print(len(input_ids),len(input_mask),len(segment_ids),len(label_id))\r\n",
        "  return {\"input_ids\":[input_ids,padding], \"input_mask\":[input_mask,padding], \"segment_ids\":[segment_ids,padding], \"label_ids\":label_id}\r\n",
        "\r\n",
        "def predict(tokenizer, predict_fn, input_str, MAX_SEQ_LENGTH):\r\n",
        "    # CONVERT DATA TO FEATURES\r\n",
        "    example_prep = ekphrasis_preprocess(input_str)\r\n",
        "    example_features = convert_single_string_to_input_dict(tokenizer=tokenizer,\r\n",
        "                                                       example_string=example_prep,\r\n",
        "                                                       max_seq_length=MAX_SEQ_LENGTH)\r\n",
        "\r\n",
        "    prediction = predict_fn(example_features)['probabilities'][0]\r\n",
        "    prediction_dict = {'POS': round(prediction[1],4), 'NEG': round(prediction[0],4)}\r\n",
        "    pprint(f\"prediction: {prediction_dict}\")\r\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoXRtSPZvdiS"
      },
      "source": [
        "# MODEL PREDICTIONS\n",
        "example_sent_neg = \"brutto e cattivo, sono veramente triste mi vorrei uccidere la mia vita non ha senso è terribile male male\"\n",
        "example_sent_pos = \"sono euforico, mi piace così tanto che sono felice solo di poter essere vivo e poter prendere il monopattino per raggiungere l'apice della mia felicità\"\n",
        "\n",
        "print(predict(tokenizer, predict_fn, example_sent_neg, MAX_SEQ_LENGTH))\n",
        "print(predict(tokenizer, predict_fn, example_sent_neg, MAX_SEQ_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKW0y32Of7Hk",
        "outputId": "2e816cc4-90c5-4b41-92f0-26069919b9c4"
      },
      "source": [
        "#PROVA PREDICT 1\r\n",
        "input = '\"input_ids\":[2, 337, 7855, 13, 32584, 49470, 29, 16, 232, 492, 122, 93, 811, 452, 12, 38, 204, 16, 56481, 30, 53, 1754, 14, 931, 60, 786, 3516, 815, 53, 3153, 12, 24973, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],' \\\r\n",
        "    '\"input_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],' \\\r\n",
        "    '\"segment_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],' \\\r\n",
        "    '\"label_ids\": [0]}'\r\n",
        "    \r\n",
        "print(input_dict)\r\n",
        "\r\n",
        "!saved_model_cli run \\\r\n",
        "    --dir 'gs://bucket-rebert/model_alberto_addestrato/1608503656' \\\r\n",
        "    --tag_set serve \\\r\n",
        "    --signature_def predict \\\r\n",
        "    --input_exprs '\"instances\":[{\"examples\":{\"input_ids\":[2, 337, 7855, 13, 32584, 49470, 29, 16, 232, 492, 122, 93, 811, 452, 12, 38, 204, 16, 56481, 30, 53, 1754, 14, 931, 60, 786, 3516, 815, 53, 3153, 12, 24973, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"input_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"segment_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"label_ids\": [0]}}]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input=[{\"input_ids\":[2, 337, 7855, 13, 32584, 49470, 29, 16, 232, 492, 122, 93, 811, 452, 12, 38, 204, 16, 56481, 30, 53, 1754, 14, 931, 60, 786, 3516, 815, 53, 3153, 12, 24973, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"input_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"segment_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\"label_ids\": [0]}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8KlAuG6LoEK",
        "outputId": "c1940379-31a3-42e3-891e-8493cb455b69"
      },
      "source": [
        "#PROVA PREDICT 2\r\n",
        "example_input = \"This is the input string\"\r\n",
        "example1 = ekphrasis_preprocess(text_processor, example_input)\r\n",
        "example2 = InputExample(guid=None,text_a = example1,text_b = None,label = 0)\r\n",
        "example3 = convert_single_example(0,example2, label_list, MAX_SEQ_LENGTH, tokenizer)\r\n",
        "print(example3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert_repo/run_classifier.py:461: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] this is the input string [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1869 721 291 43049 42225 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "<run_classifier.InputFeatures object at 0x7f637e79ee80>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzOCwFNQvyxC"
      },
      "source": [
        "  #SAVE IN BUCKLET RESULTS AND PRINT THEM\r\n",
        "output_eval_file = os.path.join(OUTPUT_DIR, \"alberto_sentipolc16_task3_results.tsv\")\r\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\r\n",
        "  print(\"***** Results *****\")\r\n",
        "  for example, prediction, id in zip(sentences_test, predictions, test_ids):\r\n",
        "    print('\\t prediction:%s \\t id:%s \\t text_a: %s' % ( np.argmax(prediction['probabilities']),str(id),str(example) ) )\r\n",
        "    writer.write(\"%s \\t %s\\n\" % (str(id), np.argmax(prediction['probabilities'])) )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}